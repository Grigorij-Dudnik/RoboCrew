# ğŸ¤– RoboCrew 

**Build AI-powered robots that see, move, and manipulate objects â€” in a few lines of code.**

RoboCrew makes it stupidly simple to create LLM agents for physical robots. Think of it like building agents with CrewAI or AutoGen, except your agents live in the real world with cameras, microphones, wheels, and arms.

![xlerobot_schema](images/main.jpg)

<div align="center">

[![PyPI version](https://badge.fury.io/py/robocrew.svg)](https://badge.fury.io/py/robocrew)
[![GitHub stars](https://img.shields.io/github/stars/Grigorij-Dudnik/RoboCrew?style=social)](https://github.com/Grigorij-Dudnik/RoboCrew)
[![Docs](https://img.shields.io/badge/docs-latest-lightblue)](https://grigorij-dudnik.github.io/RoboCrew/)
[![Discord](https://img.shields.io/static/v1?logo=discord&label=discord&message=Join&color=brightgreen)](https://discord.gg/BAe59y93)
</div>

---

## âœ¨ Features

- ğŸš— **Movement** - Pre-built wheel controls for mobile robots
- ğŸ¦¾ **Manipulation** - VLA models as tools for arms control
- ğŸ‘ï¸ **Vision** - Camera feed with automatic angle grid overlay for spatial understanding
- ğŸ¤ **Voice** - Wake-word activated voice commands and TTS responses
- ğŸ—ºï¸ **LiDAR** - Top-down mapping with LiDAR sensor
- ğŸ§  **Intelligence** - LLM agent control provides complete autonomy and decision making
- ğŸ“š **Memory** - Long-term memory to remember environment details


## ğŸ¯ How It Works

<div align="center">

<img src="https://raw.githubusercontent.com/Grigorij-Dudnik/RoboCrew/master/images/robot_agent.png" alt="How It Works Diagram" width="500">
</div>


**The RoboCrew Intelligence Loop:**

1. ğŸ‘‚ **Input** - Voice commands, text tasks, or autonomous operation
2. ğŸ§  **LLM Processing** - Gemini analyzes the task and environment
3. ğŸ› ï¸ **Tool Selection** - AI chooses appropriate tools (move, turn, grab the apple, etc.)
4. ğŸ¤– **Robot Actions** - Wheels and arms execute commands
5. ğŸ“¹ **Visual Feedback** - Cameras capture results with augmented overlay
6. ğŸ”„ **Adaptive Loop** - LLM evaluates results and adjusts strategy

This closed-loop system creates AI agents that perceive â†’ reason â†’ act, but in the physical world!


## ğŸ¨ Supported Robots

- âœ… **XLeRobot** - Full support for all features
- âœ… **LeKiwi** - Use XLeRobot code (compatible platform)
- ğŸ”œ More robot platforms coming soon! [Request your platform â†’](https://github.com/Grigorij-Dudnik/RoboCrew/issues)


## ğŸš€ Quick Start

```bash
pip install robocrew
```

### ğŸ“± Mobile Robot (XLeRobot)

```python
from robocrew.core.camera import RobotCamera
from robocrew.core.LLMAgent import LLMAgent
from robocrew.robots.XLeRobot.tools import create_move_forward, create_turn_right, create_turn_left
from robocrew.robots.XLeRobot.servo_controls import ServoControler

# ğŸ“· Set up main camera
main_camera = RobotCamera("/dev/camera_center")  # camera usb port Eg: /dev/video0

# ğŸ›ï¸ Set up servo controller
right_arm_wheel_usb = "/dev/arm_right"  # provide your right arm usb port. Eg: /dev/ttyACM1
servo_controler = ServoControler(right_arm_wheel_usb=right_arm_wheel_usb)

# ğŸ› ï¸ Set up tools
move_forward = create_move_forward(servo_controler)
turn_left = create_turn_left(servo_controler)
turn_right = create_turn_right(servo_controler)

# ğŸ¤– Initialize agent
agent = LLMAgent(
    model="google_genai:gemini-3-flash-preview",
    tools=[move_forward, turn_left, turn_right],
    main_camera=main_camera,
    servo_controler=servo_controler,
)

# ğŸ¯ Give it a task and go!
agent.task = "Approach a human."
agent.go()
```


### ğŸ¤ With Voice Commands

Add a microphone and speaker to give your robot voice commands and enable it to speak back to you:

```python
agent = LLMAgent(
    model="google_genai:gemini-3-flash-preview",
    tools=[move_forward, turn_left, turn_right],
    main_camera=main_camera,
    servo_controler=servo_controler,
    sounddevice_index=2,  # ğŸ™ï¸ provide your microphone device index
    tts=True,  # ğŸ”Š enable text-to-speech (robot can speak)
)
```

Then install Portaudio and Pyaudio for audio support:

```bash
sudo apt install portaudio19-dev
pip install pyaudio
pip install audioop-lts
```

Now just say something like **"Hey robot, bring me a beer."** â€” the robot listens continuously and when it hears the wakeword "robot" anywhere in your command, it'll use the entire phrase as its new task.

ğŸ“– **Full example:** [examples/2_xlerobot_listening_and_speaking.py](examples/2_xlerobot_listening_and_speaking.py)

---

### ğŸ¦¾ Add VLA Policy as a Tool

Let's make our robot manipulate with its arms! 

First, you need to pretrain your own policy for it - [reference here](https://xlerobot.readthedocs.io/en/latest/software/getting_started/RL_VLA.html).

After you have your policy, run the policy server in a separate terminal. Let's create a tool for the agent to enable it to use a VLA policy:

```python
from robocrew.robots.XLeRobot.tools import create_vla_single_arm_manipulation

# ğŸ¯ Create a specialized manipulation tool
pick_up_notebook = create_vla_single_arm_manipulation(
    tool_name="Grab_a_notebook",
    tool_description="Manipulation tool to grab a notebook from the table and put it to your basket.",
    task_prompt="Grab a notebook.",
    server_address="0.0.0.0:8080",
    policy_name="Grigorij/act_right-arm-grab-notebook-2",
    policy_type="act",
    arm_port=right_arm_wheel_usb,
    servo_controler=servo_controler,
    camera_config={
        "main": {"index_or_path": "/dev/camera_center"},
        "right_arm": {"index_or_path": "/dev/camera_right"}
    },
    main_camera_object=main_camera,
    policy_device="cpu",
)
```

ğŸ“– **Full example:** [examples/3_xlerobot_arm_manipulation.py](examples/3_xlerobot_arm_manipulation.py)


## ğŸ”§ Give USB Ports Constant Names (Udev Rules)

To ensure your robot's components (cameras, arms, etc.) are always mapped to the same device paths, run the following script to generate udev rules:

```bash
robocrew-setup-usb-modules
```

This script will guide you through connecting each component one by one and will create the necessary udev rules to maintain consistent device naming.

**After running the script**, you can check the generated rules at `/etc/udev/rules.d/99-robocrew.rules`, or check the symlinks:

```bash
pi@raspberrypi:~ $ ls -l /dev/arm*
lrwxrwxrwx 1 root root 7 Dec 2 11:40 /dev/arm_left -> ttyACM4
lrwxrwxrwx 1 root root 7 Dec 2 11:40 /dev/arm_right -> ttyACM2

pi@raspberrypi:~ $ ls -l /dev/cam*
lrwxrwxrwx 1 root root 6 Dec 2 11:40 /dev/camera_center -> video0
lrwxrwxrwx 1 root root 6 Dec 2 11:40 /dev/camera_right -> video2
```


## ğŸ“š Documentation

For detailed documentation, tutorials, and API references, visit our [official documentation](https://grigorij-dudnik.github.io/RoboCrew/).


## ğŸ’¬ Community & Support

- ğŸ’­ [Join our Discord](https://discord.gg/BAe59y93) - Get help, share projects, discuss features
- ğŸ“– [Read the Docs](https://grigorij-dudnik.github.io/RoboCrew/) - Comprehensive guides and API reference
- ğŸ› [Report Issues](https://github.com/Grigorij-Dudnik/RoboCrew/issues) - Found a bug? Let us know!
- â­ [Star on GitHub](https://github.com/Grigorij-Dudnik/RoboCrew) - Show your support!


## ğŸ™ Acknowledgments

Built with â¤ï¸ for the robotics and AI community. Special thanks to all contributors and early adopters!